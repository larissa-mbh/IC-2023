{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/larissa-mbh/colab-projects/blob/main/IC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT: Data Preprocessing"
      ],
      "metadata": {
        "id": "wVxQCxg65ZoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\"\n",
        "!pip install -q -U tf-models-official==2.7.0\n",
        "!pip install -U tfds-nightly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVUV5nhGgoyl",
        "outputId": "1b079995-4046-4c62-e98f-ef165edc281e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.8.3.dev202303230045-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (2.27.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (0.10.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (0.1.8)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (5.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (4.65.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (3.19.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (1.22.4)\n",
            "Collecting array-record\n",
            "  Downloading array_record-0.2.0-py39-none-any.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (1.12.0)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (1.1.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (8.1.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (1.4.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.9/dist-packages (from tfds-nightly) (2.2.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tfds-nightly) (3.15.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tfds-nightly) (4.5.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tfds-nightly) (5.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tfds-nightly) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tfds-nightly) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tfds-nightly) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tfds-nightly) (2.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from promise->tfds-nightly) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-metadata->tfds-nightly) (1.58.0)\n",
            "Installing collected packages: array-record, tfds-nightly\n",
            "Successfully installed array-record-0.2.0 tfds-nightly-4.8.3.dev202303230045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text\n",
        "!pip install tensorflow-addons\n",
        "!pip install tf-models-official"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI-f6CSG_ai0",
        "outputId": "c358f82d-dc5b-44b9-8113-aef394551b33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.9/dist-packages (2.8.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (0.13.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text) (2.8.4)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.22.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (67.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (4.5.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (23.3.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.51.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (0.31.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text) (15.0.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.40.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.16.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (6.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.9/dist-packages (0.19.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons) (4.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons) (6.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=3.6->typeguard>=2.7->tensorflow-addons) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-models-official in /usr/local/lib/python3.9/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow-text>=2.7.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.13.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.10.1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.2.2)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.5.13)\n",
            "Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.4.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.1.97)\n",
            "Requirement already satisfied: tf-slim>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.1.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.22.4)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.70.0)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.19.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (3.7.1)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.5.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (6.0)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (5.9.4)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (9.0.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (4.1.3)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.29.33)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (1.16.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (4.7.0.72)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.0.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (8.4.0)\n",
            "Requirement already satisfied: tensorflow>=2.7.0 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (2.8.4)\n",
            "Requirement already satisfied: tensorflow-model-optimization>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (0.7.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.9/dist-packages (from tf-models-official) (4.8.3)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.16.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.21.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (2.11.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from google-api-python-client>=1.6.7->tf-models-official) (4.1.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (2.27.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from kaggle>=1.3.9->tf-models-official) (2022.12.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.22.0->tf-models-official) (2022.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (23.3.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (15.0.6.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (0.31.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (1.6.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (3.19.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (4.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (2.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (1.51.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (0.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (67.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.7.0->tf-models-official) (1.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official) (0.1.8)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (4.39.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->tf-models-official) (0.11.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.9/dist-packages (from oauth2client->tf-models-official) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from oauth2client->tf-models-official) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from oauth2client->tf-models-official) (0.2.8)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (4.9.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (2022.10.31)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.9/dist-packages (from sacrebleu->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.9/dist-packages (from seqeval->tf-models-official) (1.2.2)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons->tf-models-official) (3.0.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (0.10.2)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (1.1.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->tf-models-official) (8.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.7.0->tf-models-official) (0.40.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official) (3.15.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official) (1.58.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official) (5.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle>=1.3.9->tf-models-official) (2.0.12)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (0.6.1)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.9/dist-packages (from typeguard>=2.7->tensorflow-addons->tf-models-official) (6.1.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.9/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (2.1.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.7.0->tf-models-official) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_text as text  # A dependency of the preprocessing model\n",
        "import tensorflow_addons as tfa\n",
        "from official.nlp import optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "metadata": {
        "id": "CG4QPajlNIRK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc2cc3a-22f8-4a63-a29e-5972e787785b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getCSVFromDrive(sharingLink):\n",
        "  path = 'https://drive.google.com/uc?export=download&id='+sharingLink.split('/')[-2]\n",
        "  return pd.read_csv(path, sep=\";\")"
      ],
      "metadata": {
        "id": "Ii0xdVqRSPXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/IC/PAN_style_change\n",
        "\n",
        "test_data_target = pd.read_csv('./test_target.csv', sep=\";\")\n",
        "validation_data_target = pd.read_csv('./validation_target.csv', sep=\";\")\n",
        "training_data_target = pd.read_csv('./train_target.csv', sep=\";\")\n",
        "\n",
        "test_data_text = pd.read_csv('./test_text.csv', sep=\";\")\n",
        "validation_data_text = pd.read_csv('./validation_text.csv', sep=\";\")\n",
        "training_data_text = pd.read_csv('./train_text.csv', sep=\";\")\n",
        "'''\n",
        "\n",
        "test_data_target = getCSVFromDrive('https://drive.google.com/file/d/1rEMRzvrk8VS4e_WyLIe2-19yaNKQHbnm/view?usp=share_link')\n",
        "validation_data_target = getCSVFromDrive('https://drive.google.com/file/d/1-It6bjVH-fzUFP7TYscN1RWUPbGLjux8/view?usp=share_link')\n",
        "training_data_target = getCSVFromDrive('https://drive.google.com/file/d/1-5jH9nmunpWVYttT2fYb_sOGJY5MDxWx/view?usp=share_link')\n",
        "\n",
        "test_data_text = getCSVFromDrive('https://drive.google.com/file/d/1-56fRScr4X9FjB6dBV21F79T_o77ykjA/view?usp=share_link')\n",
        "validation_data_text = getCSVFromDrive('https://drive.google.com/file/d/1-RPmnbQdFpR6xcv33HOZEUgBXFO65YgA/view?usp=share_link')\n",
        "training_data_text = getCSVFromDrive('https://drive.google.com/file/d/1-Njzm6tJ_i60Eg8CtupMl4-cZOgFbEaJ/view?usp=share_link')"
      ],
      "metadata": {
        "id": "n5BMwVM4PL8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "N3-b3S6yRWqn",
        "outputId": "0c74624c-5bdd-4ef4-89b8-270b8dfb0754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    text  \\\n",
              "0      As stephelton said in the comments to your que...   \n",
              "1      I would suspect the OS drive first or whatever...   \n",
              "2      The answer lies in SPI - Stateful Packet Inspe...   \n",
              "3      Indexing is incredibly resource intensive espe...   \n",
              "4      Do the split just once, what about someone sen...   \n",
              "...                                                  ...   \n",
              "11195  When I load this data into my data warehouse I...   \n",
              "11196  This is an attribute called origin, which may ...   \n",
              "11197  The problem is that the backup files for the t...   \n",
              "11198  There's another thing which you can do to incr...   \n",
              "11199  I am creating a dashboard in Excel in which I ...   \n",
              "\n",
              "                       file-name  \n",
              "0          ./train/problem-1.txt  \n",
              "1          ./train/problem-2.txt  \n",
              "2          ./train/problem-3.txt  \n",
              "3          ./train/problem-4.txt  \n",
              "4          ./train/problem-5.txt  \n",
              "...                          ...  \n",
              "11195  ./train/problem-11196.txt  \n",
              "11196  ./train/problem-11197.txt  \n",
              "11197  ./train/problem-11198.txt  \n",
              "11198  ./train/problem-11199.txt  \n",
              "11199  ./train/problem-11200.txt  \n",
              "\n",
              "[11200 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b6887ec7-01f2-42d1-9836-2d432cbadbad\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>file-name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>As stephelton said in the comments to your que...</td>\n",
              "      <td>./train/problem-1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I would suspect the OS drive first or whatever...</td>\n",
              "      <td>./train/problem-2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The answer lies in SPI - Stateful Packet Inspe...</td>\n",
              "      <td>./train/problem-3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Indexing is incredibly resource intensive espe...</td>\n",
              "      <td>./train/problem-4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Do the split just once, what about someone sen...</td>\n",
              "      <td>./train/problem-5.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11195</th>\n",
              "      <td>When I load this data into my data warehouse I...</td>\n",
              "      <td>./train/problem-11196.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11196</th>\n",
              "      <td>This is an attribute called origin, which may ...</td>\n",
              "      <td>./train/problem-11197.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11197</th>\n",
              "      <td>The problem is that the backup files for the t...</td>\n",
              "      <td>./train/problem-11198.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11198</th>\n",
              "      <td>There's another thing which you can do to incr...</td>\n",
              "      <td>./train/problem-11199.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11199</th>\n",
              "      <td>I am creating a dashboard in Excel in which I ...</td>\n",
              "      <td>./train/problem-11200.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11200 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b6887ec7-01f2-42d1-9836-2d432cbadbad')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b6887ec7-01f2-42d1-9836-2d432cbadbad button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b6887ec7-01f2-42d1-9836-2d432cbadbad');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "4UproHMJsYTY",
        "outputId": "156a0496-99b2-4461-daea-344d57baab8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       authors                                     site  multi-author  \\\n",
              "0            3             gamedev.stackexchange.com.7z             1   \n",
              "1            2                         superuser.com.7z             1   \n",
              "2            3                         superuser.com.7z             1   \n",
              "3            1                         superuser.com.7z             0   \n",
              "4            2          codereview.stackexchange.com.7z             1   \n",
              "...        ...                                      ...           ...   \n",
              "11195        1                 dba.stackexchange.com.7z             0   \n",
              "11196        4  networkengineering.stackexchange.com.7z             1   \n",
              "11197        4                       serverfault.com.7z             1   \n",
              "11198        2          codereview.stackexchange.com.7z             1   \n",
              "11199        4                         superuser.com.7z             1   \n",
              "\n",
              "                              file-name  \n",
              "0          ./train/truth-problem-1.json  \n",
              "1          ./train/truth-problem-2.json  \n",
              "2          ./train/truth-problem-3.json  \n",
              "3          ./train/truth-problem-4.json  \n",
              "4          ./train/truth-problem-5.json  \n",
              "...                                 ...  \n",
              "11195  ./train/truth-problem-11196.json  \n",
              "11196  ./train/truth-problem-11197.json  \n",
              "11197  ./train/truth-problem-11198.json  \n",
              "11198  ./train/truth-problem-11199.json  \n",
              "11199  ./train/truth-problem-11200.json  \n",
              "\n",
              "[11200 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aa6246c6-fd95-486c-b855-446f7edf9759\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authors</th>\n",
              "      <th>site</th>\n",
              "      <th>multi-author</th>\n",
              "      <th>file-name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>gamedev.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-1.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-2.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-3.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>0</td>\n",
              "      <td>./train/truth-problem-4.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>codereview.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-5.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11195</th>\n",
              "      <td>1</td>\n",
              "      <td>dba.stackexchange.com.7z</td>\n",
              "      <td>0</td>\n",
              "      <td>./train/truth-problem-11196.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11196</th>\n",
              "      <td>4</td>\n",
              "      <td>networkengineering.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11197.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11197</th>\n",
              "      <td>4</td>\n",
              "      <td>serverfault.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11198.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11198</th>\n",
              "      <td>2</td>\n",
              "      <td>codereview.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11199.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11199</th>\n",
              "      <td>4</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11200.json</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11200 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa6246c6-fd95-486c-b855-446f7edf9759')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aa6246c6-fd95-486c-b855-446f7edf9759 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aa6246c6-fd95-486c-b855-446f7edf9759');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# subtrair para o número de autores ficar entre [0, 3] para não dar nan no loss\n",
        "training_data_target['authors'] -= 1\n",
        "validation_data_target['authors'] -= 1\n",
        "test_data_target['authors'] -= 1"
      ],
      "metadata": {
        "id": "Po_SNzgquJI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "GrhvNlvSufqK",
        "outputId": "23eef467-94da-4380-9517-be438f3b0e19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       authors                                     site  multi-author  \\\n",
              "0            2             gamedev.stackexchange.com.7z             1   \n",
              "1            1                         superuser.com.7z             1   \n",
              "2            2                         superuser.com.7z             1   \n",
              "3            0                         superuser.com.7z             0   \n",
              "4            1          codereview.stackexchange.com.7z             1   \n",
              "...        ...                                      ...           ...   \n",
              "11195        0                 dba.stackexchange.com.7z             0   \n",
              "11196        3  networkengineering.stackexchange.com.7z             1   \n",
              "11197        3                       serverfault.com.7z             1   \n",
              "11198        1          codereview.stackexchange.com.7z             1   \n",
              "11199        3                         superuser.com.7z             1   \n",
              "\n",
              "                              file-name  \n",
              "0          ./train/truth-problem-1.json  \n",
              "1          ./train/truth-problem-2.json  \n",
              "2          ./train/truth-problem-3.json  \n",
              "3          ./train/truth-problem-4.json  \n",
              "4          ./train/truth-problem-5.json  \n",
              "...                                 ...  \n",
              "11195  ./train/truth-problem-11196.json  \n",
              "11196  ./train/truth-problem-11197.json  \n",
              "11197  ./train/truth-problem-11198.json  \n",
              "11198  ./train/truth-problem-11199.json  \n",
              "11199  ./train/truth-problem-11200.json  \n",
              "\n",
              "[11200 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17500c85-a1e5-453e-8ccc-172fd9ee5aef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>authors</th>\n",
              "      <th>site</th>\n",
              "      <th>multi-author</th>\n",
              "      <th>file-name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>gamedev.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-1.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-2.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-3.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>0</td>\n",
              "      <td>./train/truth-problem-4.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>codereview.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-5.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11195</th>\n",
              "      <td>0</td>\n",
              "      <td>dba.stackexchange.com.7z</td>\n",
              "      <td>0</td>\n",
              "      <td>./train/truth-problem-11196.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11196</th>\n",
              "      <td>3</td>\n",
              "      <td>networkengineering.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11197.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11197</th>\n",
              "      <td>3</td>\n",
              "      <td>serverfault.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11198.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11198</th>\n",
              "      <td>1</td>\n",
              "      <td>codereview.stackexchange.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11199.json</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11199</th>\n",
              "      <td>3</td>\n",
              "      <td>superuser.com.7z</td>\n",
              "      <td>1</td>\n",
              "      <td>./train/truth-problem-11200.json</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11200 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17500c85-a1e5-453e-8ccc-172fd9ee5aef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17500c85-a1e5-453e-8ccc-172fd9ee5aef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17500c85-a1e5-453e-8ccc-172fd9ee5aef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Executing eagerly? ', tf.executing_eagerly(), '\\nTensorFlow version: ', tf.__version__)\n",
        "\n",
        "test = {\n",
        "      'sentence': tf.cast(test_data_text['text'].values, tf.string),\n",
        "      'label': tf.cast(test_data_target['authors'].values, tf.int8)\n",
        "    }\n",
        "validation = {\n",
        "      'sentence': tf.cast(validation_data_text['text'].values, tf.string),\n",
        "      'label': tf.cast(validation_data_target['authors'].values, tf.int8)\n",
        "    }\n",
        "training = {\n",
        "      'sentence': tf.cast(training_data_text['text'].values, tf.string),\n",
        "      'label': tf.cast(training_data_target['authors'].values, tf.int8)\n",
        "    }\n",
        "features = ['sentence']\n",
        "test_sliceDataset = tf.data.Dataset.from_tensor_slices(test)\n",
        "validation_sliceDataset = tf.data.Dataset.from_tensor_slices(validation)\n",
        "training_sliceDataset = tf.data.Dataset.from_tensor_slices(training)\n",
        "\n",
        "print(len(list(test_sliceDataset)), ' ', len(list(validation_sliceDataset)), ' ', len(list(training_sliceDataset)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnGohd0cPOyW",
        "outputId": "bdd249ac-cd24-470b-e406-ab8012226268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing eagerly?  True \n",
            "TensorFlow version:  2.8.4\n",
            "2400   2400   11200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import os\n",
        "\n",
        "if os.environ['COLAB_TPU_ADDR']:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n",
        "  print('Using TPU')\n",
        "elif tf.config.list_physical_devices('GPU'):\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "  print('Using GPU')\n",
        "else:\n",
        "  raise ValueError('Running on CPU is not recommended.')\n",
        "'''"
      ],
      "metadata": {
        "id": "hSzxwOEQoXqr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "3e31fd24-738a-434c-e7d2-76ec9735ee0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport os\\n\\nif os.environ['COLAB_TPU_ADDR']:\\n  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\\n  tf.config.experimental_connect_to_cluster(cluster_resolver)\\n  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\\n  strategy = tf.distribute.TPUStrategy(cluster_resolver)\\n  print('Using TPU')\\nelif tf.config.list_physical_devices('GPU'):\\n  strategy = tf.distribute.MirroredStrategy()\\n  print('Using GPU')\\nelse:\\n  raise ValueError('Running on CPU is not recommended.')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model_name = 'bert_en_uncased_L-12_H-768_A-12' \n",
        "\n",
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_large/2',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_wwm_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_large':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'albert_en_xxlarge':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_large':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
        "\n",
        "print('BERT model selected           :', tfhub_handle_encoder)\n",
        "print('Preprocessing model auto-selected:', tfhub_handle_preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDiBjQ305cIb",
        "outputId": "aeb8284c-b3af-4c3f-b475-3719c2fe4c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT model selected           : https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n",
            "Preprocessing model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bert_preprocess_model(sentence_features, seq_length=128):\n",
        "  \"\"\"Returns Model mapping string features to BERT inputs.\n",
        "\n",
        "  Args:\n",
        "    sentence_features: a list with the names of string-valued features.\n",
        "    seq_length: an integer that defines the sequence length of BERT inputs.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model that can be called on a list or dict of string Tensors\n",
        "    (with the order or names, resp., given by sentence_features) and\n",
        "    returns a dict of tensors for input to BERT.\n",
        "  \"\"\"\n",
        "\n",
        "  input_segments = [\n",
        "      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n",
        "      for ft in sentence_features]\n",
        "\n",
        "  # Tokenize the text to word pieces.\n",
        "  bert_preprocess = hub.load(tfhub_handle_preprocess)\n",
        "  tokenizer = hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n",
        "  segments = [tokenizer(s) for s in input_segments]\n",
        "\n",
        "  # Optional: Trim segments in a smart way to fit seq_length.\n",
        "  # Simple cases (like this example) can skip this step and let\n",
        "  # the next step apply a default truncation to approximately equal lengths.\n",
        "  truncated_segments = segments\n",
        "\n",
        "  # Pack inputs. The details (start/end token ids, dict of output tensors)\n",
        "  # are model-dependent, so this gets loaded from the SavedModel.\n",
        "  packer = hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n",
        "                          arguments=dict(seq_length=seq_length),\n",
        "                          name='packer')\n",
        "  model_inputs = packer(truncated_segments)\n",
        "  return tf.keras.Model(input_segments, model_inputs)"
      ],
      "metadata": {
        "id": "mtwwDOAU9WPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_classifier_model(num_classes):\n",
        "\n",
        "  class Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "      super(Classifier, self).__init__(name=\"prediction\")\n",
        "      self.encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True)\n",
        "      self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "      self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, preprocessed_text):\n",
        "      encoder_outputs = self.encoder(preprocessed_text)\n",
        "      pooled_output = encoder_outputs[\"pooled_output\"]\n",
        "      x = self.dropout(pooled_output)\n",
        "      x = self.dense(x)\n",
        "      return x\n",
        "\n",
        "  model = Classifier(num_classes)\n",
        "  return model"
      ],
      "metadata": {
        "id": "4zQuz3VMWVck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "test_classifier_model = build_classifier_model(2)\n",
        "bert_raw_result = test_classifier_model(text_preprocessed)\n",
        "print(tf.sigmoid(bert_raw_result))#tf.Tensor([[0.81701595 0.2619492 ]], shape=(1, 2), dtype=float32)\n",
        "'''"
      ],
      "metadata": {
        "id": "EaAd4q4cac5I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e15a7d3d-5068-4ae8-f4bd-057a8ca61177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntest_classifier_model = build_classifier_model(2)\\nbert_raw_result = test_classifier_model(text_preprocessed)\\nprint(tf.sigmoid(bert_raw_result))#tf.Tensor([[0.81701595 0.2619492 ]], shape=(1, 2), dtype=float32)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfds_name = 'glue/cola' \n",
        "\n",
        "sentence_features = ['sentence']\n",
        "num_examples = len(list(test_sliceDataset)) + len(list(validation_sliceDataset)) + len(list(training_sliceDataset))\n",
        "\n",
        "train_split = 'train'\n",
        "validation_split = 'validation'\n",
        "test_split = 'test'\n",
        "\n",
        "target = 'authors'\n",
        "num_classes = training_data_target[target].nunique()\n",
        "\n",
        "print(f'This dataset has {num_examples} examples')\n",
        "print(f'Number of classes: {num_classes}')\n",
        "print(f'Features {sentence_features}')\n",
        "\n",
        "# The code below is just to show some samples from the selected dataset\n",
        "labels_names = ['1 autor', '2 autores', '3 autores', '4 autores']\n",
        "\n",
        "sample_i = 1\n",
        "sample_dataset = training_sliceDataset\n",
        "for sample_row in sample_dataset.take(5):\n",
        "  samples = [sample_row[feature] for feature in sentence_features]\n",
        "  print(f'sample row {sample_i}')\n",
        "  for sample in samples:\n",
        "    print(sample.numpy())\n",
        "  sample_label = sample_row['label']\n",
        "\n",
        "  print(f'label: {sample_label} ({labels_names[sample_label]})')\n",
        "  print()\n",
        "  sample_i += 1"
      ],
      "metadata": {
        "id": "jvOTAMFZmTH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1775b977-308f-41e6-8910-2b06b2d5e681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset has 16000 examples\n",
            "Number of classes: 4\n",
            "Features ['sentence']\n",
            "sample row 1\n",
            "b'As stephelton said in the comments to your question, vector math is extremely important for pretty much any 2D or 3D game. However, physics knowledge isn\\'t necessary for a lot of simple games. There are physics-like concepts you should understand a bit about, like collision, but you won\\'t need calculus or physics classes for that as long as you keep it simple. A lot of things you may want to do can be simulated simply enough that players won\\'t care much, like friction, or sliding, or gravity. A decent grasp of physics will likely help in many situations though.\\nIt is probably not required to know physics in details when you\\'re doing a game, but it definitely helps, especially if there are some \\'virtual reality\\' features in your game. A game like \"From Dust\" (Eric Chahi) is essentially physics simulation gamified, while \"Another World\" only need high-precision capture of real-life motion (so and requires little to no actual understanding of what happens).\\nIt is very likely that inertia and the other bodies motion related fields will be quite helpful to produce something that is pleasant to the eye, even if you\\'re just moving items on a chessboard. Knowing that friction exists helps using it as a game mechanic, even if you don\\'t exactly use their academic version. Same for energy conservation. In fact, I believe we could very much use game development as a way to teach physics.\\nHaving something that behaves close to our real world makes the rules of your game easier to learn for your player, feel more natural, and brings the player to the fun/challenging part more rapidly.\\nThe good approach is much simpler and simply require that the speed is increased at every frame by a constant (gravity acceleration) as stated in Newton Laws. Knowing physics makes you find the right solution faster.\\nPhysics isn\\'t really necessary unless you want to have physics in your game. While a good general knowledge of physics is recommended, it\\'s not necessary if you\\'re using someone else\\'s physics engine (which I would recommend). Also, calc based physics will get you farther. And, you really only need to know mechanics but the other things don\\'t hurt. All in all though, it depends on what you\\'re doing.\\nI\\'d be very surprised if you ever use Maxwell or Einstein theories, though. But who knows ? Ludum dare could prove me wrong.\\nThe first time I wanted to have a character jumping in a game, I started using sin(x). The result was kinda akwards: as soon as you jumped off a cliff, you\\'d end up travelling up and down, as if you\\'d be riding a curious deltaplane.'\n",
            "label: 2 (3 autores)\n",
            "\n",
            "sample row 2\n",
            "b\"I would suspect the OS drive first or whatever drive holds the swap data since you were getting the blue donut of patience. \\nTonight, while working away on my PC, it started temporarily freezing. First, I'd hear a louder-than-usual click from the hard drive bay. Then, nothing would seem out of the ordinary until I actually clicked or right-clicked on something. Then, I'd get the little spinning ring (this is on Windows 7). The mouse would still move, but Explorer became unresponsive. Sometimes, I could use Alt-Tab to get to the app switcher, but it wouldn't actually switch apps. Then, after a pause of 15 seconds to a few minutes, everything would come back to life.\\nThis is not the first time I've seen drives do this. But all of the tools I've ever used (Norton NDD, Spinrite, Ontrack, etc,) all seem to just try to read all of the sectors. But here's the weird thing: when I've seen drives fail like this, the first symptom is that they seem to, eventually, successfully perform the read (so the bad-sector scanner is happy). They just take 30 seconds to come up with a given sector.\\nBefore I write a tool to read all of the sectors for errors AND anomalous delays, I figured I'd ask: does anybody know of a tool (for Windows, Linux, or on boot-cd) which can detect per-failure symptoms like this?\\nYou can use the built in S.M.A.R.T. to get that kind of data, but the tools from the manufactures themselves are the best tools. They are also the ones you'll need to run (usually) before you can get an RMA. \\nMy strong suspicion was that this was due to a problem in one of the hard drives, but I couldn't tell which one (unless I wanted to take them all out, arrange them on the floor, and then surf the web while my ear was between them).\"\n",
            "label: 1 (2 autores)\n",
            "\n",
            "sample row 3\n",
            "b'The answer lies in SPI - Stateful Packet Inspection. This is how a router keeps track of where to send a response on its subnet. I don\\'t know all the nitty-gritty myself (I have never needed to know), but if you follow up the link you will find as much as you probably need to know. If not, a search on \"Stateful Packet Inspection\" will fill any gaps.\\nWhen the packet is received by the router, it looks at the port number and matches and sends to the computer/user.\\nThe router assigns a random unused port number and keeps a table of who the port number refers to (the user\\'s computer).\\nBut now, how does the web server manage to send the request back to the local machine? Didn\\'t the request come from an IP address on a local subnet? Where did it pick up the IP address of the local machine\\'s router? And even if it has that router\\'s IP, how does the packet get routed to the local machine once it gets to the router?\\nSo, a user makes a HTTP request to some address. That request comes from an address on a local subnet, but the router knows to send the request to its default gateway because the destination IP doesn\\'t match anything else in its routing table. The request hops through the internet and is eventually received by the destination machine (a web server). That all makes sense.\\nThis is especially needed in a NAT environment (where the internal IPs are private and external IP is constant - like sharing the internet connection).'\n",
            "label: 2 (3 autores)\n",
            "\n",
            "sample row 4\n",
            "b'Indexing is incredibly resource intensive especially when Windows is freshly installed. There are plenty of other resource intense processes such as updates that might also be going on too. It really sucks not knowing what\\'s really happening and I do wish Microsoft would make this kind of activity more visible. But they don\\'t. About your only hope in seeing what\\'s really happening would be to watch the task manager (and task bar) and read a few logs.\\nThe last thing I might point at is hardware. So are you sure your hard drive isn\\'t really the problem - or maybe a forgotten flash drive isn\\'t?! You said you had to jolt the system to unstick something. So that suggests to me that something might not be plugged in correctly or that there\\'s a failing/intermittent connection somewhere. It could be anything from a bad Ethernet jack to another drive somewhere which could be anything from a flash drive to a NAS storage device or even a different HDD partition. Then again, it could be a RAM allocation problem or even something as stupid as RAID (in the BIOS). So check that stuff too.\\nThat said, I would also suggest NOT using McAfee or Norton! Those programs are especially resource intense and don\\'t really do much of a better job than Microsoft\\'s own free Security Essentials does. In fact, McAfee and Norton don\\'t exactly totally uninstall themselves either which may be all the more reason to avoid them. And depending on who you ask, some experts might even tell you that MSE is better! So don\\'t load down the system with extra junk like redundant virus protection. Just install/enable/update MSE and your firewall and you should be good to go.\\nOne other thing you may want to look at is if you have anything else on your network. For example, if you have a file server or another desktop computer running then you may be getting attacked by a virus/worm from inside your own network. Some fancy network attached printers might also be able to do this too (don\\'t laugh). I don\\'t know if that\\'s something you really need to worry about but it has been known to happen. So the best advice there might be to shut everything else down at least until you can get Windows up and running.\\nAlso, don\\'t just install over a previous Windows installation or do the OS update process either. I\\'m not quite sure what you mean by \"complete wipe\" but if you want to truly reinstall a \"fresh\" version of Windows then you will need to completely re-partition and re-format the entire hard drive - not just wipe the Windows (aka C:) drive.'\n",
            "label: 0 (1 autor)\n",
            "\n",
            "sample row 5\n",
            "b'Do the split just once, what about someone send \"bill@@hotmail.com\"? your code will mark this as correct.\\nApparently, your code is not that \"stylish\" (you can see the PEP8 results here: http://pep8online.com/s/QbjC4Dx7)\\nIn the line where you do length = mail.split(\\'@\\'), I think the best name is parts. Just make sure you have some edge cases acknowledge.\\nIf you are writing this code because you really need a mail validator, well, I suggest you to read this interesting post. \\nRaise exceptions and use loggers, when needed. Your function returns .group() anyway, either it\\'s a valid or not valid email address. \\nIf you are compiling your regular expression there is no improvement if you do this inside your method, move your regular expression declaration outside the method so it will not be compile it again every time you call \\'email_check\\'\\nOr even better add this rule to the regular expression, like ^[a-z]{3,128}$ where it reads any characters from a to z, at least three times found and no more than 128 times.'\n",
            "label: 1 (2 autores)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list(sample_dataset))"
      ],
      "metadata": {
        "id": "0K6wiuJIqU7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cace750-a778-4a2c-e997-54088d4fe8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11200"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_configuration(number_classes):\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  metrics = tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)\n",
        "\n",
        "  return metrics, loss"
      ],
      "metadata": {
        "id": "4Unv_OddafjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinar"
      ],
      "metadata": {
        "id": "uPZ_fdG9piyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_dataset(dataset, split, batch_size, bert_preprocess_model):\n",
        "  is_training = split.startswith('train')\n",
        "\n",
        "  num_examples = len(list(dataset))\n",
        "  \n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(num_examples)\n",
        "    dataset = dataset.repeat()\n",
        "  \n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.map(lambda ex: (bert_preprocess_model(ex), ex['label']))\n",
        "  dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "  print(dataset)\n",
        "  print(num_examples)\n",
        "\n",
        "  return dataset, num_examples"
      ],
      "metadata": {
        "id": "UOCIdy53TqOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "init_lr = 2e-5\n",
        "\n",
        "print(f'Fine tuning {tfhub_handle_encoder} model')\n",
        "bert_preprocess_model = make_bert_preprocess_model(sentence_features)\n",
        "\n",
        "# USAR GPU\n",
        "strategy = tf.distribute.MirroredStrategy()\n"
      ],
      "metadata": {
        "id": "YTfv5zA5goJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb9f21c-6e71-4965-9c1b-0c678d4408c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine tuning https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3 model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "\n",
        "  # metric have to be created inside the strategy scope\n",
        "  metrics, loss = get_configuration(num_classes)\n",
        "\n",
        "  batch_size = 32\n",
        "  train_dataset, train_data_size = load_dataset(training_sliceDataset, train_split, batch_size, bert_preprocess_model)\n",
        "  validation_dataset, validation_data_size = load_dataset(validation_sliceDataset, validation_split, batch_size,\n",
        "      bert_preprocess_model)\n",
        "  test_dataset, test_data_size = load_dataset(test_sliceDataset, test_split, batch_size, bert_preprocess_model)\n",
        "  \n",
        "  # train\n",
        "  steps_per_epoch = train_data_size // batch_size\n",
        "  num_train_steps = steps_per_epoch * epochs\n",
        "  num_warmup_steps = num_train_steps // 10\n",
        "\n",
        "  # validation\n",
        "  validation_steps = validation_data_size // batch_size\n",
        "\n",
        "  # model\n",
        "  classifier_model = build_classifier_model(num_classes)\n",
        "\n",
        "  optimizer = optimization.create_optimizer(\n",
        "      init_lr=init_lr,\n",
        "      num_train_steps=num_train_steps,\n",
        "      num_warmup_steps=num_warmup_steps,\n",
        "      optimizer_type='adamw')\n",
        "\n",
        "  classifier_model.compile(optimizer=optimizer, loss=loss, metrics=[metrics])\n",
        "\n",
        "  history = classifier_model.fit(\n",
        "      x=train_dataset,\n",
        "      validation_data=validation_dataset,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      epochs=epochs,\n",
        "      validation_steps=validation_steps)"
      ],
      "metadata": {
        "id": "qlnHYCZ0hRyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbab68c-4fec-4da0-f907-75280f78c857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset element_spec=({'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'input_word_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'input_type_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int8, name=None))>\n",
            "11200\n",
            "<PrefetchDataset element_spec=({'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'input_word_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'input_type_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int8, name=None))>\n",
            "2400\n",
            "<PrefetchDataset element_spec=({'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'input_word_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None), 'input_type_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int8, name=None))>\n",
            "2400\n",
            "Epoch 1/6\n",
            "350/350 [==============================] - 351s 949ms/step - loss: 1.3956 - accuracy: 0.2997 - val_loss: 1.3113 - val_accuracy: 0.3721\n",
            "Epoch 2/6\n",
            "350/350 [==============================] - 355s 1s/step - loss: 1.3025 - accuracy: 0.3708 - val_loss: 1.2900 - val_accuracy: 0.3913\n",
            "Epoch 3/6\n",
            "350/350 [==============================] - 336s 961ms/step - loss: 1.2089 - accuracy: 0.4327 - val_loss: 1.2902 - val_accuracy: 0.3854\n",
            "Epoch 4/6\n",
            "350/350 [==============================] - 355s 1s/step - loss: 1.0697 - accuracy: 0.5144 - val_loss: 1.4322 - val_accuracy: 0.3842\n",
            "Epoch 5/6\n",
            "350/350 [==============================] - 336s 961ms/step - loss: 0.9252 - accuracy: 0.5930 - val_loss: 1.5506 - val_accuracy: 0.3571\n",
            "Epoch 6/6\n",
            "350/350 [==============================] - 355s 1s/step - loss: 0.8222 - accuracy: 0.6407 - val_loss: 1.6833 - val_accuracy: 0.3575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06xfUNS6upRG",
        "outputId": "924b9f43-0491-40b5-c48a-d9b4ef118eb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [1.3955930471420288,\n",
              "  1.3025087118148804,\n",
              "  1.2088592052459717,\n",
              "  1.0696592330932617,\n",
              "  0.9252219796180725,\n",
              "  0.8221767544746399],\n",
              " 'accuracy': [0.29973214864730835,\n",
              "  0.370803564786911,\n",
              "  0.43267858028411865,\n",
              "  0.5143749713897705,\n",
              "  0.5930356979370117,\n",
              "  0.6407142877578735],\n",
              " 'val_loss': [1.3113456964492798,\n",
              "  1.290034294128418,\n",
              "  1.2901822328567505,\n",
              "  1.4322125911712646,\n",
              "  1.550603985786438,\n",
              "  1.6833090782165527],\n",
              " 'val_accuracy': [0.3720833361148834,\n",
              "  0.39125001430511475,\n",
              "  0.3854166567325592,\n",
              "  0.3841666579246521,\n",
              "  0.3570833206176758,\n",
              "  0.35749998688697815]}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "  print(\"Evaluate on test data\")\n",
        "  results = classifier_model.evaluate(test_dataset)\n",
        "  print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYN_LfzwuvqL",
        "outputId": "c83218a6-b2be-4019-ced8-5204cd7da625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "75/75 [==============================] - 24s 277ms/step - loss: 1.6570 - accuracy: 0.3762\n",
            "test loss, test acc: [1.6570461988449097, 0.3762499988079071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportar"
      ],
      "metadata": {
        "id": "lFa94ySmpkPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main_save_path = './my_models'\n",
        "bert_type = tfhub_handle_encoder.split('/')[-2]\n",
        "saved_model_name = f'{tfds_name.replace(\"/\", \"_\")}_{bert_type}'\n",
        "\n",
        "saved_model_path = os.path.join(main_save_path, saved_model_name)\n",
        "\n",
        "preprocess_inputs = bert_preprocess_model.inputs\n",
        "bert_encoder_inputs = bert_preprocess_model(preprocess_inputs)\n",
        "bert_outputs = classifier_model(bert_encoder_inputs)\n",
        "model_for_export = tf.keras.Model(preprocess_inputs, bert_outputs)\n",
        "# Save everything on the Colab host (even the variables from TPU memory)\n",
        "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "model_for_export.save(saved_model_path, include_optimizer=False,\n",
        "                      options=save_options)\n"
      ],
      "metadata": {
        "id": "sVBosUFbplYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db92836-03a6-447d-fed2-a46d393346e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Saving', saved_model_path)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path = '/content/drive/MyDrive/Colab Notebooks/IC/PAN_style_change/Models'\n",
        "saved_model_path = path  + '/model_test'\n",
        "# Save everything on the Colab host (even the variables from TPU memory)\n",
        "save_options = tf.saved_model.SaveOptions(experimental_io_device='/job:localhost')\n",
        "model_for_export.save(saved_model_path, include_optimizer=False, options=save_options)\n",
        "#model_for_export.save(path + '/model_test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp8O0j88cFGB",
        "outputId": "01bc69a3-ca0a-4cbf-b3c1-1573f0b4b148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ./my_models/glue_cola_bert_en_uncased_L-12_H-768_A-12\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 364). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXFATmCzb1z8",
        "outputId": "c27ae6a5-7b5f-4d7c-953b-289a9fde5a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.8.0)\n",
            "Collecting keras\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.4 requires keras<2.9,>=2.8.0rc0, but you have keras 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/job:localhost'):\n",
        "  reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "metadata": {
        "id": "v2CK5v9pczge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reloaded_model = tf.keras.models.load_model(saved_model_path)"
      ],
      "metadata": {
        "id": "ADfcIPj6powQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testar"
      ],
      "metadata": {
        "id": "IMT-uCcWT_qZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare(record):\n",
        "  model_inputs = [[record[ft]] for ft in sentence_features]\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "def prepare_serving(record):\n",
        "  model_inputs = {ft: record[ft] for ft in sentence_features}\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "def print_bert_results(test, bert_result, dataset_name):\n",
        "\n",
        "  bert_result_class = tf.argmax(bert_result, axis=1)[0]\n",
        "\n",
        "  if dataset_name == 'glue/cola':\n",
        "    print('sentence:', test[0].numpy())\n",
        "    if bert_result_class == 1:\n",
        "      print('This sentence is acceptable')\n",
        "    else:\n",
        "      print('This sentence is unacceptable')\n",
        "\n",
        "  elif dataset_name == 'glue/sst2':\n",
        "    print('sentence:', test[0])\n",
        "    if bert_result_class == 1:\n",
        "      print('This sentence has POSITIVE sentiment')\n",
        "    else:\n",
        "      print('This sentence has NEGATIVE sentiment')\n",
        "\n",
        "  elif dataset_name == 'glue/mrpc':\n",
        "    print('sentence1:', test[0])\n",
        "    print('sentence2:', test[1])\n",
        "    if bert_result_class == 1:\n",
        "      print('Are a paraphrase')\n",
        "    else:\n",
        "      print('Are NOT a paraphrase')\n",
        "\n",
        "  elif dataset_name == 'glue/qqp':\n",
        "    print('question1:', test[0])\n",
        "    print('question2:', test[1])\n",
        "    if bert_result_class == 1:\n",
        "      print('Questions are similar')\n",
        "    else:\n",
        "      print('Questions are NOT similar')\n",
        "\n",
        "  elif dataset_name == 'glue/mnli':\n",
        "    print('premise   :', test[0])\n",
        "    print('hypothesis:', test[1])\n",
        "    if bert_result_class == 1:\n",
        "      print('This premise is NEUTRAL to the hypothesis')\n",
        "    elif bert_result_class == 2:\n",
        "      print('This premise CONTRADICTS the hypothesis')\n",
        "    else:\n",
        "      print('This premise ENTAILS the hypothesis')\n",
        "\n",
        "  elif dataset_name == 'glue/qnli':\n",
        "    print('question:', test[0])\n",
        "    print('sentence:', test[1])\n",
        "    if bert_result_class == 1:\n",
        "      print('The question is NOT answerable by the sentence')\n",
        "    else:\n",
        "      print('The question is answerable by the sentence')\n",
        "\n",
        "  elif dataset_name == 'glue/rte':\n",
        "    print('sentence1:', test[0])\n",
        "    print('sentence2:', test[1])\n",
        "    if bert_result_class == 1:\n",
        "      print('Sentence1 DOES NOT entails sentence2')\n",
        "    else:\n",
        "      print('Sentence1 entails sentence2')\n",
        "\n",
        "  elif dataset_name == 'glue/wnli':\n",
        "    print('sentence1:', test[0])\n",
        "    print('sentence2:', test[1])\n",
        "    if bert_result_class == 1:\n",
        "      print('Sentence1 DOES NOT entails sentence2')\n",
        "    else:\n",
        "      print('Sentence1 entails sentence2')\n",
        "\n",
        "  print('BERT raw results:', bert_result[0])\n",
        "  print()"
      ],
      "metadata": {
        "id": "HQYlQRSxprzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_bert_results_IC(test, bert_result, dataset_name):\n",
        "\n",
        "  bert_result_class = tf.argmax(bert_result, axis=1)[0]\n",
        "\n",
        "  print('Dataset ', dataset_name)\n",
        "  print('sentence:', test[0].numpy())\n",
        "  if bert_result_class == 0:\n",
        "    print('Um autor')\n",
        "  else:\n",
        "    print('Múltiplos autores')\n",
        "\n",
        "  print('BERT raw results:', bert_result[0])\n",
        "  print()"
      ],
      "metadata": {
        "id": "kv3NFH-5j66z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/job:localhost'):\n",
        "  for test_row in test_sliceDataset.shuffle(1000).map(prepare).take(5):\n",
        "    if len(sentence_features) == 1:\n",
        "      result = reloaded_model(test_row[0])\n",
        "    else:\n",
        "      result = reloaded_model(list(test_row))\n",
        "\n",
        "    print_bert_results_IC(test_row, result, tfds_name)"
      ],
      "metadata": {
        "id": "ILZNOs-Up52C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847cbe1e-d7d6-4202-9241-8adb9778894e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset  glue/cola\n",
            "sentence: [b\"Surprisingly, if you do this correctly and responsibly, most ISPs will cooperate with you. They won't identify the user to you, but they will deal with the user.\\nYou also might want to look into banning the IPs of free, publicly available proxy servers and prohibit signing up with a list of known free webmail providers to cut down on his options, but even then, a cheap account with a VPN provider and/or an obscure webmail provider can bypass those.\\nDoing all of the above is somewhat effective, but a determined troll will get through those blocks, and in my experience, the only thing that really works is moderating new user sign ups, and that causes issues with driving off legitimate new users.  You could just keep banning him until he tires and finds something else to capture his interest (like a shiny object).\\nYou can use cookies, Flash cookies or browser fingerprinting to try to remember him no matter his IP, but those are fairly easy to bypass.  A more advanced, but equally easy to bypass ban (provided he knows what to look for) is to capture his MAC address and ban that.\\nReport him to his ISP. Include detailed logs (IPs, actions, timestamps with timezones), copies of your site's administrative policy, and so on. If his ISP is not cooperative, ban his entire ISP. If users complain, tell them that you are very sorry, but they have an ISP that refuses to deal with abuse.\\nWhen a new user signs up, use a service like Twilio to send a text message to their phone containing a short passcode.  They have to correctly enter this code to verify their account.  This is similar to how Craigslist (attempts) to keep spammers out and is also frequently used by online banking as a kind of a poor man's two-factor authentication system.\\nIf he's truly that big a problem, you can use cookies and/or browser fingerprinting to ID him, and redirect him to a copy of the site that only he can see or make his posts and activities only readable to him - so he can troll away until he gets bored, thinking he's bypassing the ban, but in actuality, you've created a private little sandbox just for him, and no one else is impacted by his crap.  I've done that in the past with some more troublesome forum trolls, but unless they're really, really persistent and really, really vile, it's more effort than it's worth, IMHO.\\nMy first suggestion, is if you have his original IP logged, to report him to his ISP for abuse, and try to get his service yanked, or suspended.  This will vary based on the ISP, but they can be very helpful.  Failing that...\"]\n",
            "Um autor\n",
            "BERT raw results: tf.Tensor([ 0.46886867 -0.3329084  -0.59736687 -0.19962835], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: [b'Nowadays the biggest problems are compatibility and performance. Your game might work well on your i7 CPU on a fast browser, but will be totally unplayable on a netbook with internet explorer. Also, Microsoft has a good history of promoting their own technologies, IE10 might not go the WebGL way in order to promote DirectX, especially since you can code apps for Windows with web languages. You need to define on what system you\\'re planning your game to play on, if you focus on a particular system you can optimize towards it and use the technologies it supports. Keep also in mind that if the game is not stored locally you will need to take the bandwidth into account.\\nWhat are the graphics capabilities of Javascript, can it only render simple 2d graphics (like a tetris or pacman game), or can it be used for a fast shooting game or a car racing game as well? Do the graphics run smoothly or are they laggy? And what about 3d graphics?\\nNo problem playing sounds if the browser supports HTML5 Audio API, which can be accessed with javascript and doesn\\'t require a proper html tag.\\nI\\'m hearing about the new HTML 5 features which allow graphics to be rendered, audio to be played, etc. My question is, does that mean Javascript is ready for a graphics intensive game, or is the better option still to use Flash or Java?\\nTechnically, everything would be ready for a neat 2D game, for 3D you can either go with HTML5 and a 3D engine, or start your way up with WebGL.\\nSo I\\'d say yes it is ready, but it greatly depends on your targets and if you are willing to spend a lot of time to optimize to the last bit every line and events of your code. And don\\'t dream about simply putting your game online, thousands of users will not understand why their IE cannot run your game.\\nSame question with the audio capabilities, are they laggy, smooth? Can you smoothly play music in the background and also play sounds in response to user actions (like when he shoots a gun)?\\nThe answer is hard to define and is more a question you should ask yourself depending on the game you\\'re about to create.\\n2D games are usually working smoothly on almost any system and can be used to create iOS simple games, with some limits of course. Depending on the game you don\\'t necessarily need HTML5 Canvas and you could first try with plain old \"DHTML\".']\n",
            "Múltiplos autores\n",
            "BERT raw results: tf.Tensor([-1.0522616  -0.3061437   0.65814817  0.94239676], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: [b'Just in case you want to rely on tools included in Windows, feel free to check the Event Log -> Application for \"CertificateServicesClient-AutoEnrollment\" with ID 64 and a criticality of \"Warning\".\\nAlternatively, you can get a detailed error code using the Windows Update Minitool, which is basically a 3rd party tool interfacing with Windows Update.\\nAnd despite Kaspersky telling you to get the certificate store backup from a working device, feel free to just get the latest list of certificates from Microsoft.\\nIn the software you can choose your OS, download ALL updates for that version of windows, and then start the updater, and it will install all missing updates.\\nFor me, both Windows Update and the Windows Store seemingly started downloading but got stuck at \"Download pending\" and would not continue or show an error code.\\nConsidering that Windows 7 and Windows 10 are pretty much suffering from similar decisions when looking at Windows Update, I\\'d like to contribute a further possible solution.\\nFor some reason, I was not able to use those certificates without rootsupd.exe - the certificate management applet would show the certificate list, but would deem it broken.\\nTurn off automatic Windows updates from the Control Panel and turn off the Windows Update service. Then, go to C:\\\\Windows\\\\SoftwareDistribution\\\\Download and delete everything in the folder. Restart the Windows Update service. Finally, open the CLI, type wuauclt.exe /updatenow, and press \"Enter.\" Try to download the patches again and see what happens.\\nWhen I used the rootsupd.exe to extract and import the contents of authrootstl.cab, my Windows Update finally started working again. ']\n",
            "Um autor\n",
            "BERT raw results: tf.Tensor([ 0.4045084  -0.03142013 -0.4938119  -0.09542765], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: [b\"Whoever wrote bytesToHex is probably more experienced with PHP than Java, because otherwise they would know that building a string in a loop like that is inexcusable for someone who's been programming Java for more than two weeks. Given that the size of the output is known from the start, I'd be inclined to just build it into a char[] and then call the String(char[]) constructor rather than use a StringBuilder.\\nReuse of IV. In certain applications this is excusable, but as a general rule you should use a different, randomly generated, IV for each message and send it (unencrypted) along with the message. Otherwise if your messages start out with the same text then an eavesdropper can get information on where they diverge.\\nBad padding. If you look at that code carefully you'll see that it pads with spaces on encryption and doesn't unpad on decryption. So you'll have to unpad in the next layer up, and if the encrypted string ends in one or more spaces you'll unpad incorrectly. I would remove padString and the call to it, and change the cipher algorithm to AES/CBC/PKCS7 (assuming that it's supported; if not, try AES/CBC/PKCS5).\\nAs upper item has explained, I can't get why you use 2 * IV_LENGTH to get iv which is wrong. For example, use a iv of 16 bytes, which is 128 bits and encodingToString() change it to 24 bytes(128 / 6 = 22 + 2 paddings, for reasons see here). So it is obvious not 2 * IV_LENGTH. Try to change to split, see details at item one.\\nFrom Base64's name, you can derive that its base is 64, ie 2**6, so for your iv length(16 * 8) can't be divided with no reminder, so encodeToString() will add padding to you iv. To split two parts of base64 of iv and cipher text conveniently, you may add a separator char larger than 63(], for example) and not special char of regex(for in java split() receive a regex string) so that you can easily split them.\"]\n",
            "Múltiplos autores\n",
            "BERT raw results: tf.Tensor([-0.11988571  0.0827082  -0.00173272 -0.02178797], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: [b\"Is there some difference between a VirtualBox virtual machine and a real bare metal system that I need to account for? And is there a way to salvage my current virtual reference system without having to remake it from scratch on bare metal?\\nXP did not ship with AHCI support, nearly every modern computer comes with this feature and the option is generally enabled.\\n(3) However, after using Symantec Ghost 11.5 to make an image of the system and deploying it to a real physical machine, the Windows XP sysprepped system fails to boot. Specifically, the system keep restarting even before the Windows logo shows up.\\nThis is likely down to ATA / AHCI support. On the baremetal computer you're attempting to install, open the bios settings dialog and check to see if you can reconfigure SATA to be ATA instead of AHCI.\\n(2) I ran sysprep to reseal the system for deployment. By restarting the virtual machine directly I saw that sysprep ran as configured and the system was correctly deployed.\\n(1) I created a VirtualBox virtual machine and installed then customized a Windows XP SP3 reference system in it.\"]\n",
            "Um autor\n",
            "BERT raw results: tf.Tensor([ 0.3228394   0.28960901 -0.6273545  -0.87170976], shape=(4,), dtype=float32)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/job:localhost'):\n",
        "  serving_model = reloaded_model.signatures['serving_default']\n",
        "  for test_row in test_sliceDataset.shuffle(1000).map(prepare_serving).take(5):\n",
        "    result = serving_model(**test_row)\n",
        "    # The 'prediction' key is the classifier's defined model name.\n",
        "    print_bert_results_IC(list(test_row.values()), result['prediction'], tfds_name)"
      ],
      "metadata": {
        "id": "7DtA-d1wp71Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea04c4b3-30ed-4694-fe9f-a2170c1b03f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset  glue/cola\n",
            "sentence: b'I want to use an AMD Ryzen 2400g, but I will be using OpenCL on three discrete AMD Radeon RX580s installed in it. Will the Vega compute units on the APU still be available if I install the Radeons? Or does their installation disable the integrated vega GPU?\\nthere is a limit of 1 gpu however, so if your plan was to use it for mining, likely the cost would just not be worth the attempt, better off to get something like a Ryzen 1200 and a full size x370 or something like that (more pci-e slots)\\nRyzen 2200/2400g with a x8 link speed is basically enough bandwidth leftover to feed any current single gpu from AMD or Nvidia up to about Titan V or possibly some dual gpu on 1 card configurations.....they really should have given the ability to disable the Vega graphics core to shift between the iGP and dGPU as needed, maybe this will come with BIOS updates, however, like I said and I have been spending quite a few hours researching this for my own information, you ABSOLUTELY can pair up Ryzen 2200/2400G with a discrete graphics card.\\nfor external graphics (if one so chooses) it is not easy to do so by all means, but claiming it is not possible is not at all truthful.'\n",
            "Múltiplos autores\n",
            "BERT raw results: tf.Tensor([-1.9344941   0.61149263  1.3084852   0.89628035], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: b\"Well it could be the ocean! But probably not. Unfortunately, it seems @billy never came back here, so we'd never know.\\nThe problem is that every time I opened google maps on my browser (any webgl capable browser, I suppose), the screen turned blue-ish and it remained that way until the maps tab was open. It went back to the nice warm shade it was when that tab was closed.\\nHere's my relevant post on the Google Maps forum that led me to some useful information on the issue: https://productforums.google.com/forum/#!topic/maps/bDGTgfSwM7k;context-place=forum/maps\\nIt's quite an old question but I had similar problems recently while using google maps on my MBP (late 2011) and on searching google maps screen turns blue, this came up as one of the top results (1st on google); so I suppose this answer would at least help point people looking for my issue in a helpful direction (a proxy question FWIW ;)\\nI would have posted this as a comment to the question but I don't have enough points to do that, so here it is. ;)\\nCertain areas of the Google earth map I view have a blue haze over areas and I suspect but am not certain this could be flood zones or zones where recent flooding took place.\"\n",
            "Um autor\n",
            "BERT raw results: tf.Tensor([ 0.73211724 -0.21336061 -1.2965087  -0.7530275 ], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: b'For some reason, if zabbix can\\'t run a remote command, then it leaves the trigger with an \"OK\" status.  Is there any way to get this to switch to a \"PROBLEM\" status?\\nI have a problem with some zabbix triggers not firing due to the fact that EnableRemoteCommands hasn\\'t been enabled on certain hosts.  I tried to address this by adding a trigger specifically checking whether EnableRemoteCommands is set to 1 in the zabbix agent config:\\nBut, of course, this trigger itself relies on remote commands, so won\\'t run on hosts which has them disabled.  \\nIt will fire if item will return nothing, e.g. EnableRemoteCommands is disabled. Please don\\'t use system.run when you absolutely don\\'t need to, it\\'s disabled by default by purpose \\xe2\\x80\\x94 you can do anything using other ways Zabbix provides you with.\\nIf the main agent configuration is done in one file only, we could probably make use of vfs.file.regexp item (or vfs.file.regmatch) here. For instance:\\nThis is not perfect though, because it only searches the main configuration file for EnableRemoteCommands setting, but this setting may be overridden in an included file.\\nUserParameter=zabbix.remotecommands, egrep \\'EnableRemoteCommands.*=.*1\\' /etc/zabbix/zabbix_agentd.conf'\n",
            "Múltiplos autores\n",
            "BERT raw results: tf.Tensor([ 0.12707426  0.65069205 -0.2752118  -0.7581166 ], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: b\"Also, I regularly run sudo apt-get autoremove and sudo apt-get clean to keep the package manager cleaned up.\\nThose are the major aspects that you must be aware of. The rest of the notes are mentioned in the previous answers.\\nSoftware updates, like Muhammad pointed, are generally optional (updating for the sake of updating only isn't good, because you're updating something that just works). However, security updates are a must.\\nGenerally, you maintain a Linux system by reading the logs, maybe also having triggers for some events. Then you choose an appropriate action strategy.\\nRunning a file integrity checker like aide is a good idea too. Maybe a rootkit scanner as well, if you feel yourself at danger.\\nReading the system logs (e.g. via System > Administration > Log File Viewer) has turned out to be especially important, since often performance problems that I'm experiencing have had a root cause obvious (though not necessarily obviously fixable) in the logs.\\nBy large files I assume you mean logfiles that tend to grow. Linux mitigates this by rotating the logs using logrotate.\"\n",
            "Múltiplos autores\n",
            "BERT raw results: tf.Tensor([-0.68964547  0.27048218  0.4405668   0.43958306], shape=(4,), dtype=float32)\n",
            "\n",
            "Dataset  glue/cola\n",
            "sentence: b'Save your changes and issue the command update-initramfs \\xe2\\x80\\x93u and reboot. With any luck you should be up and working.\\nThe article recommends adding the following four lines to the end of the /etc/initramfs-tools/modules file:\\nUnfortunately, the setup for these drivers do not work with the current Hyper-V integration components that are included with the Linux kernel.\\nTake a look at the following blog post - it is for Ubuntu 10.10 server specifically, though the steps illustrated should get you most of the way there, if not completely:\\nUbuntu does not appear to be an OS that supports the \"Enlightenment\" virtualization support that is \"preferred\" with Hyper-V. It\\'s not alone as most earlier version of Windows also fall into the same situation. If you search around, there are some hacks/guides which may help.\\nhttp://blogs.msdn.com/b/virtual_pc_guy/archive/2010/10/21/installing-ubuntu-server-10-10-on-hyper-v.aspx\\nSynthetic Mouse Support is not included in the Hyper-V Linux integration components.  There are references in the older, separate builds of the integration components (2.1) for the Citrix Project Satori Web site at http://www.xen.org/products/satori.html '\n",
            "Múltiplos autores\n",
            "BERT raw results: tf.Tensor([-0.22937751 -0.0926135  -0.24190447  0.5415065 ], shape=(4,), dtype=float32)\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}